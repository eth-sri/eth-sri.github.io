---
title: Deep Learning for Big Code
ref: bigcode23
description: Graduate seminar on new methods and systems for learning from programs.
semester: Spring 2023
number: 263-2926-00L
lecturer: Dr. Veselin Raychev
ta: Marc Fischer, Mark Niklas Müller, Nikola Jovanović, Mislav Balunović, Maximilian Baader, Dr. Timon Gehr
assistants:
edoz: https://www.vorlesungen.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?lerneinheitId=168505&semkez=2023S&ansicht=LEHRVERANSTALTUNGEN&lang=en
session-time-place: Mon 16-18, CAB G52 (not an online course, certain events will also be available over zoom, check your email for link)
lecture-time-place:
exercise-time-place:
credits: 2
image: assets/images/orator.jpg

---

<h2>Overview</h2>

<p>The objective of the seminar is to:</br>
	<ul>
		<li>Introduce students to the field of Deep Learning for Big Code.</li>
		<li>Learn how machine learning models can be used to solve practical challenges in software engineering and programming beyond traditional methods.</li>
		<li>Highlight the latest research and work opportunities in industry and academia available on this topic.</li>
	</ul>
</p>

<p>The seminar is carried out as a set of presentations (2 each lecture) chosen from a set of available papers (available below). The grade is determined as a function of the presentation, handling questions and answers, and participation:</p>


<h2>Schedule</h2>

<table centering border="0" width="100%" cellspacing="10" cellpadding="5">
<th width="12%">Date</th><th width=55%>Title</th><th width=15%>Presenter</th><th width=5%>Slides</th><th width=13%>Advisor</th>

<tr>
	<td>20.02</td>
	<td>Introduction to the seminar (topics, objectives, structure):</td>
	<td>Veselin Raychev</td>
	<td>
	<a href="https://files.sri.inf.ethz.ch/website/teaching/bigcode2023/bigcode23.pdf" class="pdf"><img src="{{"/assets/icons/icon-slides.svg" | relative_url}}" class="svg-icon" border="0" alt="PDF"></a>
	</td>
	<td></td>
	</tr>

	<tr>
		<td></td>
		<td>List of papers to choose from:</td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2111.05711.pdf" target="_blank">Counterfactual Explanations for Models of Code</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/1906.07181.pdf" target="_blank">Learning Execution through Neural Code Fusion</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2106.15339.pdf" target="_blank">SpreadsheetCoder: Formula Prediction from Semi-structured Context</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://www.cis.upenn.edu/~mhnaik/papers/iclr22.pdf" target="_blank">CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2008.02944.pdf" target="_blank">Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://www.cs.columbia.edu/~suman/docs/trex_final.pdf" target="_blank">Trex: Learning execution semantics from micro-traces for binary similarity</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2010.07987.pdf" target="_blank">Empirical Study of Transformers for Source Code</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2002.04694.pdf" target="_blank">Adversarial Robustness for Code</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/1803.06686.pdf" target="_blank">Code Vectors: Understanding Programs Through Embedded Abstracted Symbolic Traces</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://www.software-lab.org/publications/icse2021_IdBench.pdf" target="_blank">IdBench: Evaluating Semantic Representations of Identifier Names in Source Code</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2203.07814.pdf" target="_blank">Competition-level code generation with alphacode</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2006.03511.pdf" target="_blank">Unsupervised translation of programming languages</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2106.06981.pdf" target="_blank">Thinking Like Transformers</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2301.13196.pdf" target="_blank">Looped Transformers as Programmable Computers</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://miltos.allamanis.com/publicationfiles/allamanis2020typilus/allamanis2020typilus.pdf" target="_blank">Typilus: Neural Type Hints</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://www.software-lab.org/publications/fse2020_TypeWriter.pdf" target="_blank">Typewriter: Neural type prediction with search-based validation</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://www.cis.upenn.edu/~mhnaik/papers/iclr20.pdf" target="_blank">Hoppity: Learning graph transformations to detect and fix bugs in programs</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://www.software-lab.org/publications/icse2019_NL2Type.pdf" target="_blank">NL2Type: inferring JavaScript function types from natural language information</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2112.02969.pdf" target="_blank">Jigsaw: Large language models meet program synthesis</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://sigmodrecord.org/publications/sigmodRecord/2203/pdfs/04_tp-bao-marcus.pdf" target="_blank">Bao: Making learned query optimization practical</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2003.07914.pdf" target="_blank">Big code!= big vocabulary: Open-vocabulary models for source code</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2003.13848.pdf" target="_blank">Code Prediction by Feeding Trees to Transformers</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/1902.06111.pdf" target="_blank">Getafix: Learning to fix bugs automatically</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2106.06600.pdf" target="_blank">Break-It-Fix-It: Unsupervised Learning for Program Repair</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://proceedings.mlr.press/v162/he22a/he22a.pdf" target="_blank">On distribution shift in learning-based bug detectors</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2208.11640.pdf" target="_blank">Repair is nearly generation: Multilingual program repair with llms</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td></td>
		<td><a href="https://arxiv.org/pdf/2201.11227.pdf" target="_blank">Synchromesh: Reliable code generation from pre-trained language models</a></td>
		<td></td>
		<td></td>
		<td></td>
	</tr>

</table>
