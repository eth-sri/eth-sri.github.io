---
title: Reliable and Interpretable Artificial Intelligence
ref: riai2018
description: Graduate course involving synthesis, neural network analysis and probabilistic programming.
semester: Fall 2018
number: 263-2400-00L
lecturer: Prof. Dr. Martin Vechev
ta: Dr. Petar Tsankov, Dr. Dana Drachsler-Cohen, Benjamin Bichsel, Samuel Steffen, Gagandeep Singh
assistants: 
edoz: http://www.vvz.ethz.ch/Vorlesungsverzeichnis/lerneinheit.view?semkez=2018W&ansicht=KATALOGDATEN&lerneinheitId=123752&lang=en
session-time-place: 
lecture-time-place: Mon 10-12, HG D 7.1
exercise-time-place: Mon 13-14, LFW C4 or Wed 11-12 CAB G59
credits: 4
image: ai.png
---

<h2>Overview</h2>

<p>Creating reliable and explainable probabilistic models is a major challenge to solving the artificial intelligence problem. This course covers some of the latest and most exciting advances that bring us closer to constructing such models. To gain a deeper understanding of the material and be able to apply and extend the concepts, an important part of the course will be a group hands-on programming project where students will build a system based on the learned material. While we do cover the latest material, the course should be self-contained and any necessary background will be introduced in the lectures or in exercise sessions (e.g., basics of deep learning) together with additional pointers if needed. The course covers the following inter-connected directions:
</p>

<p>
<strong>Part I: Robust and Explainable Deep Learning</strong></br>
Deep learning technology has made impressive advances in recent years. Despite this progress however, the fundamental challenge with deep learning remains that of understanding what a trained neural network has actually learned, and how stable that solution is. For example: is the network stable to slight perturbations of the input (e.g., an image)? How easy it is to fool the network into mis-classifying obvious inputs? Can we guide the network in a manner beyond simple labeled data?
</br>
Topics:
</br>
<ul>
	<li>Attacks: Finding adversarial examples via state-of-the-art attacks (e.g., FGSM, PGD attacks).</li>
	<li>Defenses: Automated methods and tools which guarantee robustness of deep nets (e.g., using abstract domains, mixed-integer solvers)</li>
	<li>Combing differentiable logic with gradient-based methods so to train networks to satisfy richer properties.</li>
	<li>Frameworks: AI2, DiffAI, Reluplex, DQL, DeepPoly, etc.</li>
</ul>
</p>

<p>
<strong>Part II: Program Synthesis/Induction</strong></br>
Synthesis is a new frontier in AI where the computer programs itself via user provided examples. Synthesis has significant applications for non-programmers as well as for programmers where it can provide massive productivity increase (e.g., wrangling for data scientists). Modern synthesis techniques excel at learning functions over discrete spaces from (partial) intent. There have been a number of recent, exciting breakthroughs in techniques that discover complex, interpretable/explainable functions from few examples, partial sketches and other forms of supervision. 
</br>
Topics:
</br>
<ul>
	<li>Theory of program synthesis: version spaces, counter-example guided inductive synthesis (CEGIS) with SAT/SMT, lower bounds on learning.</li>
	<li>Applications of techniques: synthesis for end users (e.g., spreadsheets) and data analytics.</li>
	<li>Combining synthesis with learning: application to learning from code.</li>
	<li>Frameworks: PHOG, DeepCode. </li>
</ul>
</p>

<p>
<strong>Part III: Probabilistic Programming</strong></br>
Probabilistic programming is an emerging direction, recently also pushed by various companies (e.g., Facebook, Uber, Google) whose goal is democratize the construction of probabilistic models. In probabilistic programming, the user specifies a model while inference is left to the underlying solver. The idea is that the higher level of abstraction makes it easier to express, understand and reason about probabilistic models. 
</br>
Topics:
</br>
<ul>
	<li>Probabilistic Inference: sampling based, exact symbolic inference, semantics</li>
	<li>Applications of probabilistic programming: bias in deep learning, differential privacy (connects to Part I).</li>
	<li>Frameworks: PSI, Edward2, Venture.</li>
</ul>
</p>

<h2>Lectures</h2>
<table centering border="0" width="100%" cellspacing="0" cellpadding="0">
<th width=7%>No.</th> <th width="10%">Date</th><th width=43%>Content</th><th width="12%">Slides</th><th width=15%> Exercises </th><th width=13%> Solutions </th>

<tr>
<td>1</td>
<td>Sept 24</td>
<td>Introduction</td>
<td><a href="https://files.sri.inf.ethz.ch/website/teaching/riai2018/materials/slides/intro.pdf" class="pdf" title="intro"><img src="/assets/icons/icon-slides.svg" class="svg-icon" border="0" alt="PDF"></a></td>
<td></td>
<td></td>
</tr>
</table>

