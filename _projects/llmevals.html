---
title: Reliable Language Model Evaluation
ref: llmevals
image: svg/llmevals.svg
description: "We study the principles of LLM evaluation and benchmarking, pointing out limitations in current methodologies, proposing new approaches for more reliable evaluations, and developing new benchmarks."

---

<h2>Live Leaderboards</h2>

<div class="grid">
  {% include card.html
    title="MathArena: Uncontaminated Math Competitions"
    output="vertical"
    url="https://matharena.ai"
    image="/assets/projects/matharena.png"
  %}
  {% include card.html
    title="BaxBench: Secure and Correct Backends"
    output="vertical"
    url="https://baxbench.com/#leaderboard-section"
    image="/assets/projects/baxbench.png"
  %}
  {% include card.html
    title="SWT-Bench: Assessing Test-writing Capabilities"
    output="vertical"
    url="https://swtbench.com/?results=verified"
    image="/assets/projects/swtbench.png"
  %}
  {% include card.html
    title="EU AI Act Compliance Leaderboard"
    output="vertical"
    url="https://huggingface.co/spaces/latticeflow/compl-ai-board"
    image="/assets/projects/complai.png"
  %}
</div>

<h2>Publications</h2>

{% include get-publications.html filter="project" key="llmevals" %}